\section{UI evaluation} \label{UIEval}
To evaluate the User interface of our system, a usability test was conducted using participants that matches our target users. From this test we wanted to determine if our system provides a good user experience, and whether the users felt that it was an improvement of the current system.
To do this we used the SUS questions as a basis for our questionnaire, with some added questions in the same format as SUS. The added questions are:
\begin{itemize}
    \item I found the error descriptions clear enough.
    \item I think the system could be an improvement to the course.
\end{itemize}
The participants of the usability test were students on the first semester of their Software masters degree who were at the time taking the Programming Paradigms course. 
These students have a good understanding of how to use haskell to solve simple exercises, and based on this we expected them to be able to successfully use our system. They also have experience with how the course was run, and were therefore able to give feedback on whether it was an improvement.

The usability test was conducted in a group room with other students present but not interacting with the participant, this simulated a class setting. 
The equipment used was the participants own laptops, which simulated use in a real world setting.

\subsection*{Tasks}
To conduct the usability test, we created a series of tasks that the participants had to complete, in order to experience all the features of the system.
The tasks that were presented to the participants were the following:
\begin{enumerate}
    \item Create an account and log-in
    \item In this semester, find exercise "only two" in the "functions and lists" problem set.
    \item Sumbit a program that you expect to not solve the exercise
    \item Submit a program that solves the exercise.
    \item Check which exercises has been completed by you.
\end{enumerate}


\subsection*{Results} \label{UiResults}
After each user test, the participants were given the questionnaire described in section \ref{SUSScore}. An average SUS score was then calculated, allowing us to estimate how well the website functions from a userâ€™s point of view. Based on the questionaires the SUS score for our website is \texttt{82.5}
According to Adobe a score of 75 is considered good, and a score of 85,5 is considered excelent \cite{adobeSUS}. Our systems average score was 82.5 putting it close to an excelent score. 

Furthermore, each participants was asked two additional questions on top of the standard questionnaire as described in section \ref{UIEval}. The average score for each of the questions can be seen in the table below:
\begin{table}[H]
\centering
\begin{tabular}{lllll}
\cline{1-2}
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Question}           & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Average Score} &  &  &  \\ \cline{1-2}
\multicolumn{1}{|l|}{Error description is clear}                 & \multicolumn{1}{l|}{4.7}                                    &  &  &  \\ \cline{1-2}
\multicolumn{1}{|l|}{The overall system could be an improvement} & \multicolumn{1}{l|}{4.3}                                    &  &  &  \\ \cline{1-2}
                                                                 &                                                           &  &  & 
\end{tabular}
\end{table}
The results show that, on average, the participants felt that the system provided error messages that were clear. 
It also showed that, on average, the participants agreed that the system would be an improvement to the course.