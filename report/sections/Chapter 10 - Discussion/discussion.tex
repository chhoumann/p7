This project aims to create a teaching platform to facilitate learning with active feedback among lecturers and students. 
By providing a user-friendly interface for creating and solving exercises, as well as a system for managing and organizing the exercises, the platform seeks to enhance the educational experience for students in the \textit{Programming Paradigms} course. 
In this discussion, we will explore the features and functionality of the platform, as well as its potential impact on the teaching and learning process.
One of the key features of the platform is its ability to support the creation of exercises, allowing students to submit solution attempts and receive feedback quickly.
The platform's user-friendly interface makes it easy for lecturers to create and organize exercises, and to specify the tests that evaluate student answers. This not only allows lecturers to tailor exercises to the specific needs and abilities of their students, but also to provide detailed feedback on student performance, which can help students to improve their understanding and skills.

The platform was designed and implemented as a multi layered architecture based on a client-server structure with an additional API responsible for testing the code submissions and a database for persisting data. 
One of the primary goals of the platform was to ensure its ability to handle a relatively large number of concurrent users and exercise submissions. 
To achieve this, we implemented several versions of the Test Runner system in order to gauge the efficiency of each approach. 
The initial version of the Test Runner system did not contribute sufficiently towards scalability.
It was built to establish a baseline to compare against future improvements and as a proof of concept. With this first version, it was clear that the synchronous behavior was too slow and unable to serve even a small group of students.
To address this, we implemented a queueing and sweeping system in subsequent asynchronous versions of the system. Doing so greatly improved the efficiency of the system, ensured less memory usage, and consequently made the system more scalable, allowing it to handle a larger number of users. 

In order to determine the performance and scalability of the different versions of the system, we constructed a benchmarking suite. 
This benchmarking suite was designed to stress test the system. 
This was done by simulating different amounts of requests, at the same time, with different configurations the given system version. The benchmarking was based on testing for the upper limit of realistic scenarios that the system would encounter during normal usage, for example in a class room environment. 
This way of benchmarking gave us critical insight into how to optimally configure the system based on the load that it was expected to handle, and thus achieve further benefits alongside the improvements following the introduction of the queue and sweeping system. A limiting factor in the benchmarking was the fact that the increments, for example the amount threads, were too large. 
A better approach would be to find the most optimal initial configuration, and testing different configurations that were only slightly different in order to gauge how that would affect the performance. 

In addition to benchmarking, we also conducted a usability test with students from the \textit{Programming Paradigms} course, to evaulate the user interface. To conduct this usability test, we provided them with a questionnaire based on the \textit{System Usability Scale} (SUS). 
The participants of the usability test gave the platform a high SUS score, indicating that the platform is usable to such a degree that it could be deployed for use in the class room. The participants responses particularly indicated that the platforms ability to display useful error messages on code submissions, were very helpful. 
They also indicated that the system would improve the \textit{Programming Paradigms} course. This result does have limitations as the pool of students participating in the questionnaire was small. 
Similarly, no lecturers were given a questionnaire, which limits the credibility of the test. We do, however feel, that the initial results are promising and indicate that such a platform could be useful in the classroom.
 

