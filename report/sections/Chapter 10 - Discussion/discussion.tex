This project aims to create a teaching platform to facilitate learning with active feedback among lecturers and students.
By providing a user-friendly interface for creating and solving exercises, as well as a system for managing and organizing the exercises, the platform seeks to enhance the educational experience for students in the \textit{Programming Paradigms} course.
In this discussion, we will explore the features and functionality of the platform, as well as its potential impact on the teaching and learning process.
One of the key features of the platform is its ability to support the creation of exercises, allowing students to submit solution attempts and receive feedback quickly.

The platform's user-friendly interface makes it easy for lecturers to create and organize exercises, and to specify the tests that evaluate student answers. This not only allows lecturers to tailor exercises to the specific needs and abilities of their students, but also to provide detailed feedback on student performance, which can help students to improve their understanding and skills.
We conducted a usability test with students from the \textit{Programming Paradigms} course to evaluate the user interface.
To conduct this usability test, we provided them with a questionnaire based on the \textit{System Usability Scale} (SUS).
The participants of the usability test gave the platform a high SUS score, indicating that the platform is usable to such a degree that it could be deployed for use in the classroom. The participants responses particularly indicated that the platform's ability to display useful error messages on code submissions was very helpful.
They also indicated that the system would improve the \textit{Programming Paradigms} course.
It is worth noting that the results of the usability test may not be perfectly representative as the number of participating students was small.
Similarly, no lecturers were given a questionnaire, which limits the credibility of the test.
We do, however, feel that these results are promising and indicate that the platform could be useful in a practical classroom setting.
In section \ref{sec:MoSCoW}, we listed a number of prioritized features for a platform of this type.
As mentioned, we planned to only implement the features listed in the \textit{Must have} section of the MoSCoW analysis.
However, we also managed to implement the features mentioned in the \textit{Should have} and \textit{Could have} sections.
Consequently, the resulting product more closely resembled what we initially had envisioned for the final product, making it feasible for use in practice, as supported by the results of usability test.

To further support practical application, the platform was designed and implemented as a multi layered architecture based on a client-server structure with an additional API responsible for testing the code submissions and a database for persisting data.
One of the primary goals of the platform was to ensure its ability to handle a relatively large number of concurrent users and exercise submissions.
To achieve this, we implemented several versions of the Test Runner system in order to gauge the efficiency of each approach.
The initial version of the Test Runner system did not contribute sufficiently towards scalability; it was built to establish a baseline to compare against future improvements and as a proof of concept.
With this first version, it was clear that the synchronous behavior was too slow and unable to serve even a small group of students.
To address this, we implemented a queueing and sweeping system in subsequent asynchronous versions of the system.
Doing so greatly improved the efficiency of the system, ensured less memory usage, and consequently made the system more scalable, allowing it to handle a larger number of users.
This approach was inspired by the agile methodology described in section \ref{sec:agile-dev}.
Because we developed the platform in smaller increments, we were able to change our approach to the project as we organically gained new insights and knowledge.
This resulted in a working MVP early on, which we could iterate and improve on over time.
The iterative process also allowed us to test each iteration from which we were able to deduce inefficiencies and problems.
This gave us more concrete insight into what we should direct our attention to.

In order to determine the performance and scalability of the different versions of the system, we constructed a benchmarking suite for stress testing.
The goal with benchmarking was to test system behavior in realistic scenarios, for example in a classroom environment.
This way of benchmarking gave us critical insight into how to optimally configure the system based on the load that it was expected to handle, and thus achieve further benefits alongside the improvements following the introduction of the queue and sweeping system.
A limiting factor in the benchmarking was the fact that each benchmark was performed with preset values for the configuration variables with a somewhat large increment between each preset.
We did this because benchmarking once already took over a day, so we had to impose some restrictions for the values used in benchmarking.
A better approach would be to find the most optimal initial configuration, and testing different configurations that were only slightly different in order to gauge how that would affect the performance.

Both the usability test and the benchmarking shows that the final product would be usable in a practical classroom setting to further improve the learning experience in the \textit{Programming Paradigms} course.
This satisfies the use cases presented in section \ref{sec:use_cases}, and consequently our product lives up to the requirements presented in chapter \ref{chap:Specification}.