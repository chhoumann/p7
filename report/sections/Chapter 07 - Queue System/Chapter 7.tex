\chapter{Queue System} \label{chap:QueueSystem}
In chapter \ref{chap:TestRunner}, we described how running multiple GHC interpreter processes asynchronously eventually crashes the test runner web server given sufficiently many simultaneous requests.
As mentioned, we needed a way to limit the number of running GHC interpreter processes in order to prevent crashes.
In the following sections, we first give a general overview of how this was implemented.
We then describe the technical approach in more detail.

\section{Overview}
To solve the aforementioned problem, we decided to implement a queue system which enqueues requests for future processing if some upper limit is reached.
However, this introduces a new problem.
The server can no longer directly respond to a POST request with the test runner results because the request may be handled at a later point in time.
This means that if the request were enqueued for long enough, it would eventually time out.
In order to solve this, the test runner web server sends back a unique token to the client immediately upon receiving a POST request.
This token serves as a ticket for the client.
Using it, the client can continuously poll the server for the result of processing the request.
To achieve this, we implemented a new endpoint which expects the token as a parameter.
There are three possible responses from this endpoint:
\begin{enumerate}
    \item If the given token is not present within the queue, the response body contains a special "not found" message.
    \item If the given token is present within the queue but has not yet been processed, the response body contains a special "in progress" message.
    \item If the given token is present within the queue and has been processed, the response body contains a special "complete" message as well as the results of running the tests on the submitted code from the request.
\end{enumerate}

In summary, the test runner web server features two endpoints.
The first endpoint expects a POST request containing a code submission which it enqueues into a worker queue along.
It then responds to the client with a unique token (UUID).
This UUID is also used by the web server as a key to look up the given code submissions in the queue.
The second endpoint expects a GET request with a UUID as a parameter that is then used to look up the result of processing the code submission with the matching UUID.
It then responds to the client with the result of processing that submission if it was found.

\section{The queue}
We have two endpoints that need to share data, and the Rust programming language does not allow global variables.
Therefore, we opted to create a shared state data structure using the \texttt{layer} feature in the axum framework.
The \texttt{layer} feature is axum's default implementation of middleware for processing data across a group of endpoints.
In our case, there are two key variables inside the shared state structure that both endpoints need access to:
\begin{enumerate}
    \item \textbf{A sender channel:} In practice, this channel is used as a worker queue in which code submission requests are enqueued for processing. It has a corresponding \textbf{receiver channel}, which is to dequeue and schedule work. Importantly, requests sent via the sender channel are received in a first-in-first-out order.
    \item \textbf{The job results hashmap:} Upon first receiving a code submission request, a UUID is generated. This is used as a key inside the job results hashmap, where the value is initially set to Rust \texttt{Option} value of type \texttt{TestRunnerResult} --- in other words, a key-value pair with the UUID referencing an empty job result, indicating that the work has not yet been completed. An \texttt{Option} type is simply a nullable data structure in Rust; an optional value. When the test runner has processed the code submission, the result is inserted as the value of the \texttt{Option} type. This value can then be looked up when the client requests the result processing their code submission, since the client is provided with the UUID upon sending a code submission, as previously mentioned.
\end{enumerate}

In order to limit the number of GHC interpreter processes, a thread pool with a specified size is created whose responsibility is to process code submissions.
These threads have access to the aforementioned receiver channel containing work to be processed.
The creation and content of the the thread pools can be seen in code snippet \ref{lst:worker-thread}.

\begin{lstlisting}[language=rust, escapechar=|, caption={Rust code showing allocation of thread pools and scheduling of code submission processing.}, label={lst:worker-thread}]
async fn worker_thread(
    rx: Receiver<TestRunnerWork>,
    job_results: Arc<Mutex<Box<HashMap<Uuid, Option<TestRunnerResult>>>>>,
    limit: usize
) {
    let stream = tokio_stream::wrappers::ReceiverStream::new(rx);

    stream.for_each_concurrent(limit, |work| async {
        debug!("Running worker thread on UUID {}...", work.id); |\label{line:concurrent-start}|

        let res = schedule_test(work.submission).await;
        let mut map = job_results.lock().unwrap();
        map.insert(work.id, Some(res)); |\label{line:concurrent-end}|
    }).await;
}
\end{lstlisting}

We use the \texttt{futures} crate in Rust to allocate some number of threads dictated by the variable \texttt{limit} of type \texttt{usize}.
The receiver channel is converted to a \texttt{ReceiverStream} using the \texttt{tokio\_stream} utility crate.
A \texttt{ReceiverStream} is simply a wrapper which implements the \texttt{Stream} trait from the \texttt{futures} crate.
This allows us to combine functionality from the two crates.
In practice, implementations of the \texttt{Stream} trait simply provide a stream of values asynchronously --- essentially an asynchronous iterable.

Using this, we can call \texttt{for\_each\_concurrent} from the \texttt{futures} crate to run a number of tasks asynchronously but with the aforementioned limit.
The \texttt{futures} framework then manages the thread pool automatically, and we only need to specifiy the work that the threads need to do, which can be seen in lines \ref{line:concurrent-start} --- \ref{line:concurrent-end}.

The \texttt{schedule\_test} function calls the test runner, which is responsible for running the GHC interpreter and returning the results of running the tests on the submitted code from the client, as described in chapter \ref{chap:TestRunner}.
Once the result is computed, the thread acquires the mutex lock to the hashmap or blocks until it can acquire the lock.
It then inserts the result into the hashmap by using the UUID as the key.
Once the \texttt{map} variable goes out of scope, the mutex is unlocked so other threads may access the hashmap.

In summary, a specified number of concurrent threads in a thread pool continuously receive code submissions from the \texttt{ReceiverStream} channel.
Each submission is then processed by one of these threads, and the result of the test runner is packaged into the hashmap where the UUID serves as a key to look up the result.

While this approach solved the initial problem of only being able to process one code submission at a time, a new problem is the fact that the hashmap continuously grows over time --- data is only ever added to the hashmap and never removed.
Consequently, the memory use of the test runner would eventually exceed its capacity, resulting in a crash.
In the following section, we introduce a sweeping system as a solution to this problem.

\section{Sweeping system}
Data is added to the job results hashmap every time a client posts a code submission.
As a reminder, this is simply a key-value pair consisting of a UUID and a Rust \texttt{Option} of type \texttt{TestRunnerResult}.
To solve the problem of the continuously growing size of the hashmap, our first idea was to remove data from the hashmap when the client polls for it.
In other words, the data is kept in memory \textit{until} the client requests it, at which point it is returned to the client through a HTTP response and subsequently removed from the job results hashmap.
While this worked fine under ideal circumstances, two potential issues with this approach may arise in practice:
\begin{itemize}
    \item The result being removed immediately upon being requested means that if some client-side error occurs, the client is unable to request the data again.
    \item If the client posts a code submission but never polls for the result (say, as a result of losing connection), the data still continues to remain in memory.
\end{itemize}

Since this idea would only work under ideal circumstances, we opted for a safer solution inspired by the garbage collectors and the concept of sweeping.
The idea is to sweep old data from the hashmap when new data is added, but only if long enough time has passed since the last time sweeping was performed.
This ensures that sweeping will not happen several times every second if multiple clients were to post a code submission at the same time.
To achieve this, we introduced a new \texttt{timestamp} field to the \texttt{TestRunnerResult} data structure, which is set when a thread finishes processing a code submission.
This timestamp simply denotes the point in time at it was added to the job results hashmap.
Then, outdated data are removed from the hashmap if the difference between its timestamp and the current time is greater than some lifetime set through an environment variable.
Code snippet \ref{lst:sweeping} shows the same function as code snippet \ref{lst:worker-thread}, however lines \ref{line:last-sweep-time} --- \ref{line:sweep-config} and lines \ref{line:sweep-start} --- \ref{line:sweep-end} have been introduced to implement the sweeping system.


\begin{lstlisting}[language=rust, escapechar=~, caption={Temp}, label={lst:sweeping}]
async fn worker_thread(
    rx: Receiver<TestRunnerWork>,
    job_results: Arc<Mutex<Box<HashMap<Uuid, Option<TestRunnerResult>>>>>,
    limit: usize
) {
    let stream = tokio_stream::wrappers::ReceiverStream::new(rx);
    let last_sweep_time = Arc::new(Mutex::new(SystemTime::now())); ~\label{line:last-sweep-time}~
    let config = sweep_configuration::from_dot_env(); ~\label{line:sweep-config}~

    stream.for_each_concurrent(limit, |work| async {
        let res = schedule_test(work.submission).await;
        let mut map = job_results.lock().unwrap();
        map.insert(work.id, Some(res));

        if last_sweep_time.lock().unwrap().elapsed().unwrap() > ~\label{line:sweep-start}~
            config.duration_between_sweeps {
            map.retain(|&_, res| {
                if res.is_none() {
                    return true
                }

                res.as_ref().unwrap().timestamp.elapsed().unwrap() <=
                    config.lifetime
            });

            *last_sweep_time.lock().unwrap() = SystemTime::now();
        } ~\label{line:sweep-end}~
    }).await;
}
\end{lstlisting}

Line \ref{line:last-sweep-time} initializes the last sweeping time variable as the current time, and wraps it in a mutex to prevent race conditions from arising.
Line \ref{line:sweep-config} loads the sweeping configuration data structure by reading defined values from a \texttt{.env} file using the helper function \texttt{from\_dot\_env()}.
The configurable variables include the duration between sweeps as well as the lifetime of data inside the hashmap.
Lines \ref{line:sweep-start} --- \ref{line:sweep-end} is the sweeping algorithm itself.
For each element in the hashmap where the \texttt{Option} type has a value, job results are removed if their elapsed time exceeds the configured lifetime.
This is achieved using the \texttt{retain()} function, a function that retains elements in the hashmap if and only if they fulfill some predicate.

This sweeping system achieves the desired result of removing job result data from the hashmap to prevent it from continuously growing while still retaining the data for some time, allowing clients to request the same data multiple times should an error occur.
Furthermore, one can tweak the aforementioned environment variables to best suit the circumstances under which the system is used.
For example, lowering the duration between sweeps would result in the sweeping algorithm running more frequently, decreasing the system's memory usage but increasing the CPU usage.