\section{User Interface Evaluation} \label{UIEval}
To evaluate the user interface of our system, a usability test was conducted using participants that matches one of our target users described in section \ref{sec:target-users}.
The participants of the usability test were students on the first semester of their Software masters degree who, at the time, were taking the \textit{Programming Paradigms} course.
From this test, we wanted to determine if our system provides a good user experience for students, and whether the students felt that it was an improvement of the current system.
To do this, we used the SUS questions as a basis for our questionnaire, with some added questions in the same format as SUS. The added questions were:
\begin{itemize}
    \item I found the error descriptions clear enough.
    \item I think the system could be an improvement to the course.
\end{itemize}

The usability test was conducted in a group room with other students present but not interacting with the participant, this simulated a class setting.
The equipment used was the participants own laptops, which simulated use in a real world setting.

To conduct the usability test, we created a series of tasks that the participants had to complete, in order to experience all the features of the system.
The tasks that were presented to the participants were the following:
\begin{enumerate}
    \item Create an account and log-in.
    \item In this semester, find exercise "only two" in the "functions and lists" problem set.
    \item Submit a program that you expect to not solve the exercise.
    \item Submit a program that solves the exercise.
    \item Check which exercises has been completed by you.
\end{enumerate}


\subsection{Results} \label{UiResults}
After each user test, the participants were given the questionnaire described in section \ref{sec:SUSScore}.
An average SUS score was then calculated, allowing us to estimate how well the website functions from a user's perspective.
Based on the questionnaires the, SUS score for our website is \texttt{82.5}, close to an excellent score.

Furthermore, each participants was asked two additional questions on top of the standard questionnaire. The average score for the two added questions can be seen in the table below:
\begin{table}[H]
\centering
\begin{tabular}{lllll}
\cline{1-2}
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Question}           & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Average Score} &  &  &  \\ \cline{1-2}
\multicolumn{1}{|l|}{Error description is clear}                 & \multicolumn{1}{l|}{4.7}                                    &  &  &  \\ \cline{1-2}
\multicolumn{1}{|l|}{The overall system could be an improvement} & \multicolumn{1}{l|}{4.3}                                    &  &  &  \\ \cline{1-2}
                                                                 &                                                           &  &  &
\end{tabular}
\end{table}
The results show that, on average, the participants felt that the system provided error messages that were clear.
It also showed that, on average, the participants agreed that the system would be an improvement to the course.